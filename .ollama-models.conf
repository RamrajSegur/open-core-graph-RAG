# Ollama Models Configuration
# This file defines which models to download for the competitive NER system
# Format: MODEL_NAME (will be pulled via 'ollama pull MODEL_NAME')

# Phase 1: Recommended for competitive NER (3 models, ~12 GB total)
# Download these to start with Phase 1 setup
llama2
neural-chat

# Phase 2: Additional models for higher accuracy (optional, ~21 GB total)
# Uncomment to download these models
# llama2:13b
# orca

# Phase 3: State-of-the-art local model (optional, ~26 GB)
# Warning: Large model, requires significant resources
# dolphin-mixtral

# Notes:
# - mistral is already downloaded by default (4.4 GB)
# - Each model is downloaded once and cached locally
# - Models are stored in ~/ollama-models/ (bound to /root/.ollama/models in container)
# - Blank lines and lines starting with '#' are ignored
